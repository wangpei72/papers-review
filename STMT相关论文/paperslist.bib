@article{Ma2020,
abstract = {Natural language processing (NLP) models have been increasingly used in sensitive application domains including credit scoring, insurance, and loan assessment. Hence, it is critical to know that the decisions made by NLP models are free of unfair bias toward certain subpopulation groups. In this paper, we propose a novel framework employing metamorphic testing, a well-established software testing scheme, to test NLP models and find discriminatory inputs that provoke fairness violations. Furthermore, inspired by recent breakthroughs in the certified robustness of machine learning, we formulate NLP model fairness in a practical setting as ($\epsilon$, k)-fairness and accordingly smooth the model predictions to mitigate fairness violations. We demonstrate our technique using popular (commercial) NLP models, and successfully flag thousands of discriminatory inputs that can cause fairness violations. We further enhance the evaluated models by adding certified fairness guarantee at a modest cost.},
author = {Ma, Pingchuan and Wang, Shuai and Liu, Jin},
doi = {10.24963/ijcai.2020/64},
file = {:home/wangpei/papers-review/STMT相关论文/Metamorphic Testing and Certified Mitigation of Fairness Violations in NLP.pdf:pdf},
keywords = {AI Ethics: Fairness,Natural Language Processing: NLP Applications and Tools},
pages = {458--465},
title = {{Metamorphic Testing and Certified Mitigation of Fairness Violations in NLP Models}},
year = {2020}
}
@article{Zhou2018,
abstract = {Automated machine translation software and services have become widely available and increasingly popular. Due to the complexity and flexibility of natural languages, automated testing and quality assessment of this type of software is extremely challenging, especially in the absence of a human oracle or a reference translation. Furthermore, even if a reference translation is available, some major evaluation metrics, such as BLEU, are not reliable on short sentences, the type of sentence now prevailing on the Internet. To alleviate these problems, we have been using a metamorphic testing technique to test machine translation services in a fully automatic way without the involvement of any human assessor or reference translation. This article reports on our progress, and presents some interesting preliminary experimental results that reveal quality issues of English-to-Chinese translations in two mainstream machine translation services: Google Translate and Microsoft Translator. These preliminary results demonstrate the usefulness and potential of metamorphic testing for applications in the natural language processing domain.},
author = {Zhou, Zhi Quan and Sun, Liqun},
doi = {10.1109/ASWEC.2018.00021},
file = {:home/wangpei/papers-review/STMT相关论文/MetamorphicTestingforMachineTranslations-MT4MT.pdf:pdf},
isbn = {9781728112411},
journal = {Proceedings - 25th Australasian Software Engineering Conference, ASWEC 2018},
keywords = {MT4MT,Machine translation,Metamorphic testing,Oracle problem,Quality evaluation,Software testing},
number = {January},
pages = {96--100},
publisher = {IEEE},
title = {{Metamorphic testing for machine translations: MT4MT}},
year = {2018}
}
@article{Murphy2009,
abstract = {It is challenging to test applications and functions for which the correct output for arbitrary input cannot be known in advance, e.g. some computational science or machine learning applications. In the absence of a test oracle, one approach to testing these applications is to use metamorphic testing: existing test case input is modified to produce new test cases in such a manner that, when given the new input, the application should produce an output that can be easily be computed based on the original output. That is, if input x produces output f(x), then we create input x́ such that we can predict f(x́) based on f(x); if the application or function does not produce the expected output, then a defect must exist, and either f(x) or f(x́) (or both) is wrong. By using metamorphic testing, we are able to provide built-in "pseudo-oracles" for these so-called "nontestable programs" that have no test oracles. In this paper, we describe an approach in which a function's metamorphic properties are specified using an extension to the Java Modeling Language (JML), a behavioral interface specification language that is used to support the "design by contract" paradigm in Java applications. Our implementation, called Corduroy, pre-processes these specifications and generates test code that can be executed using JML runtime assertion checking, for ensuring that the specifications hold during program execution. In addition to presenting our approach and implementation, we also describe our findings from case studies in which we apply our technique to applications without test oracles.},
author = {Murphy, Christian and Shen, Kuang and Kaiser, Gail},
doi = {10.1109/ICST.2009.19},
file = {:home/wangpei/papers-review/STMT相关论文/Using JML Runtime Assertion Checking to Automate Metamorphic Testing in
Applications without Test Oracles.pdf:pdf},
isbn = {9780769536019},
journal = {Proceedings - 2nd International Conference on Software Testing, Verification, and Validation, ICST 2009},
pages = {436--445},
publisher = {IEEE},
title = {{Using JML runtime assertion checking to automate metamorphic testing in applications without test oracles}},
year = {2009}
}
@article{Zhang2020,
abstract = {This paper provides a comprehensive survey of Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.},
archivePrefix = {arXiv},
arxivId = {1906.10742},
author = {Zhang, Jie M. and Harman, Mark and Ma, Lei and Liu, Yang},
doi = {10.1109/tse.2019.2962027},
eprint = {1906.10742},
file = {:home/wangpei/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2020 - Machine Learning Testing Survey, Landscapes and Horizons(3).pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
number = {X},
pages = {1--1},
title = {{Machine Learning Testing: Survey, Landscapes and Horizons}},
volume = {X},
year = {2020}
}
@article{Pesu2018,
abstract = {With the growing popularity of machine translation services, it has become increasingly important to be able to assess their quality. However, the test oracle problem makes it difficult to conduct automated testing. In this paper, we propose a Monte Carlo method, in combination with metamorphic testing, to overcome the oracle problem. Using this method, we assessed the quality of three popular machine translation services' namely, Google Translate, Microsoft Translator, and Youdao Translate. We set the source language to be English, and the target languages included Chinese, French, Japanese, Korean, Portuguese, Russian, Spanish, and Swedish. A sample of 33,600 observations (involving a total of 100,800 actual translations) was collected and analyzed using a 3 56 factorial design. Based on this data, our model found Google Translate to be the best (in terms of the metamorphic relation used) for each and every target language considered. A trend for Indo- European languages producing better results was also identified.},
author = {Pesu, Daniel and Zhen, Jingfeng and Zhou, Zhi Quan and Towey, Dave},
doi = {10.1145/3193977.3193980},
file = {:home/wangpei/papers-review/STMT相关论文/A Monte Carlo Method for Metamorphic Testing
of Machine Translation Services.pdf:pdf},
isbn = {9781450357296},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {Machine translation quality,Monte Carlo method,metamorphic testing,natural languages,oracle problem},
pages = {38--45},
publisher = {ACM},
title = {{A Monte Carlo Method for Metamorphic Testing of Machine Translation Services}},
year = {2018}
}
@article{Wang2019,
abstract = {Despite getting widely adopted recently, a Neural Machine Translation (NMT) system is often found to produce translation failures in the outputs. Developers have been relying on in-house system testing for quality assurance of NMT. This testing methodology requires human-constructed reference translations as the ground truth (test oracle) for example natural language inputs. The testing methodology has shown benefits of quickly enhancing an NMT system in early development stages. However, in industrial settings, it is desirable to detect translation failures without reliance on reference translations for enabling further improvements on translation quality in both industrial development and production environments. Aiming for a practical and scalable solution to such demand in the industrial settings, in this paper, we propose a new approach for automatically identifying translation failures without requiring reference translations for a translation task. Our approach focuses on a property of natural language translation that can be checked systematically by using information from both the test inputs (i.e., the texts to be translated) and the test outputs (i.e., the translations under inspection) of the NMT system. Our evaluation conducted on real-world datasets shows that our approach can effectively detect property violations as translation failures. By deploying our approach in the translation service of WeChat (a messenger app with more than one billion monthly active users), we show that our approach is both practical and scalable in the industrial settings.},
author = {Wang, Wenyu and Zheng, Wujie and Liu, Dian and Zhang, Changrong and Zeng, Qinsong and Deng, Yuetang and Yang, Wei and He, Pinjia and Xie, Tao},
doi = {10.1109/DSN-Industry.2019.00007},
file = {:home/wangpei/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2019 - Detecting Failures of Neural Machine Translation in the Absence of Reference Translations.pdf:pdf},
isbn = {9781728130323},
journal = {Proceedings - 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks - DSN 2019 Industry Track},
keywords = {ML quality assurance,failure detection,neural machine translation},
pages = {1--4},
publisher = {IEEE},
title = {{Detecting Failures of Neural Machine Translation in the Absence of Reference Translations}},
year = {2019}
}
@article{Lee2020,
author = {Lee, Dickson T S},
file = {:home/wangpei/papers-review/STMT相关论文/Metamorphic Robustness Testing of Google Translate.pdf:pdf},
isbn = {9781450379632},
keywords = {acm reference format,machine translation,meta-,metamorphic testing,morphic robustness testing,mt4mt,oracle problem,robustness testing},
pages = {388--395},
title = {{Metamorphic Robustness Testing of Google Translate}},
year = {2020}
}
@techreport{He,
abstract = {Machine translation software has seen rapid progress in recent years due to the advancement of deep neural networks. People routinely use machine translation software in their daily lives, such as ordering food in a foreign restaurant, receiving medical diagnosis and treatment from foreign doctors, and reading international political news online. However, due to the complexity and intractability of the underlying neural networks, modern machine translation software is still far from robust. To address this problem , we introduce referentially transparent inputs (RTIs), a simple, widely applicable methodology for validating machine translation software. A referentially transparent input is a piece of text that should have invariant translation when used in different contexts. Our practical implementation, Purity, detects when this invariance property is broken by a translation. To evaluate RTI, we use Purity to test Google Translate and Bing Microsoft Translator with 200 un-labeled sentences, which led to 123 and 142 erroneous translations with high precision (79.3{\%} and 78.3{\%}). The translation errors are diverse, including under-translation, over-translation, word/phrase mistranslation, incorrect modification, and unclear logic. These translation errors could lead to misunderstanding, financial loss, threats to personal safety and health, and political conflicts.},
archivePrefix = {arXiv},
arxivId = {2004.10361v1},
author = {He, Pinjia and Meister, Clara and Su, Zhendong},
eprint = {2004.10361v1},
file = {:home/wangpei/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He, Meister, Su - Unknown - Testing Machine Translation via Referential Transparency.pdf:pdf},
title = {{Testing Machine Translation via Referential Transparency}}
}
@article{Saha2018,
abstract = {Metamorphic testing is a well known approach to tackle the oracle problem in software testing. This technique requires the use of source test cases that serve as seeds for the generation of follow-up test cases. Systematic design of test cases is crucial for the test quality. Thus, source test case generation strategy can make a big impact on the fault detection effectiveness of metamorphic testing. Most of the previous studies on metamorphic testing have used either random test data or existing test cases as source test cases. There has been limited research done on systematic source test case generation for metamorphic testing. This paper provides a comprehensive evaluation on the impact of source test case generation techniques on the fault finding effectiveness of metamorphic testing. We evaluated the effectiveness of line coverage, branch coverage, weak mutation and random test generation strategies for source test case generation. The experiments are conducted with 77 methods from 4 open source code repositories. Our results show that by systematically creating source test cases, we can significantly increase the fault finding effectiveness of metamorphic testing. Further, in this paper we introduce a simple metamorphic testing tool called "METtester" that we use to conduct metamorphic testing on these methods.},
author = {Saha, Prashanta and Kanewala, Upulee},
doi = {10.1145/3193977.3193982},
file = {:home/wangpei/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Saha, Kanewala - 2018 - Fault Detection Effectiveness of Source Test Case Generation Strategies for Metamorphic Testing.pdf:pdf},
isbn = {9781450357296},
journal = {IEEE/ACM International Workshop on Metamorphic Testing},
keywords = {Branch coverage,Line coverage,Metamorphic testing,Random testing,Source test case generation,Weak mutation},
publisher = {ACM},
title = {{Fault Detection Effectiveness of Source Test Case Generation Strategies for Metamorphic Testing}},
url = {https://doi.org/10.1145/3193977.3193982},
volume = {18},
year = {2018}
}
@article{Zhang,
abstract = {This paper introduces Mutamorphic Relation for Machine Learning Testing. Mutamorphic Relation combines data mutation and metamorphic relations as test oracles for machine learning systems. These oracles can help achieve fully automatic testing as well as automatic repair of the machine learning models. The paper takes TransRepair as an example to show the effectiveness of Mutamorphic Relation in automatically testing and improving machine translators, TransRepair detects inconsistency bugs without access to human oracles. It then adopts probability-reference or cross-reference to post-process the translations, in a grey-box or black-box manner, to repair the inconsistencies. Manual inspection indicates that the translations repaired by TransRepair improve consistency in 87{\%} of cases (degrading it in 2{\%}), and that the repairs of have better translation acceptability in 27{\%} of the cases (worse in 8{\%}).},
archivePrefix = {arXiv},
arxivId = {1902.01509},
author = {Zhang, Jie M},
doi = {10.1145/3387940.3391541},
eprint = {1902.01509},
file = {:home/wangpei/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang - Unknown - Automatic Improvement of Machine Translation Using Mutamorphic Relation (Invited Talk Paper).pdf:pdf},
isbn = {9781450379632},
keywords = {metamorphic testing,mutamorphic relation,mutation testing},
publisher = {ACM},
title = {{Automatic Improvement of Machine Translation Using Mutamorphic Relation (Invited Talk Paper)}},
url = {https://doi.org/10.1145/3387940.3391541}
}
@techreport{Sun,
abstract = {This paper presents TransRepair, a fully automatic approach for testing and repairing the consistency of machine translation systems. TransRepair combines mutation with metamorphic testing to detect inconsistency bugs (without access to human oracles). It then adopts probability-reference or cross-reference to post-process the translations, in a grey-box or black-box manner, to repair the inconsistencies. Our evaluation on two state-of-the-art translators, Google Translate and Transformer, indicates that TransRepair has a high precision (99{\%}) on generating input pairs with consistent translations. With these tests, using automatic consistency metrics and manual assessment, we find that Google Translate and Transformer have approximately 36{\%} and 40{\%} inconsistency bugs. Black-box repair fixes 28{\%} and 19{\%} bugs on average for Google Translate and Transformer. Grey-box repair fixes 30{\%} bugs on average for Transformer. Manual inspection indicates that the translations repaired by our approach improve consistency in 87{\%} of cases (degrading it in 2{\%}), and that our repairs have better translation acceptability in 27{\%} of the cases (worse in 8{\%}).},
archivePrefix = {arXiv},
arxivId = {1910.02688v2},
author = {Sun, Zeyu and Zhang, Jie M and Harman, Mark and Papadakis, Mike and Zhang, Lu},
eprint = {1910.02688v2},
file = {:home/wangpei/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun et al. - Unknown - Automatic Testing and Improvement of Machine Translation.pdf:pdf},
keywords = {machine translation,testing and repair,translation consistency},
title = {{Automatic Testing and Improvement of Machine Translation}}
}
@article{Jia2007,
author = {Jia, Yue and Member, Student and Member, Mark Harman},
file = {:home/wangpei/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jia, Member, Member - 2007 - An Analysis and Survey of the Development of Mutation Testing.pdf:pdf},
journal = {IEEE Transactions on Software Engineering},
pages = {1--27},
title = {{An Analysis and Survey of the Development of Mutation Testing}},
year = {2007}
}
@article{He2019,
abstract = {In recent years, machine translation software has increasingly been integrated into our daily lives. People routinely use machine translation for various applications, such as describing symptoms to a foreign doctor and reading political news in a foreign language. However, the complexity and intractability of neural machine translation (NMT) models that power modern machine translation make the robustness of these systems difficult to even assess, much less guarantee. Machine translation systems can return inferior results that lead to misunderstanding, medical misdiagnoses, threats to personal safety, or political conflicts. Despite its apparent importance, validating the robustness of machine translation systems is very difficult and has, therefore, been much under-explored. To tackle this challenge, we introduce structure-invariant testing (SIT), a novel metamorphic testing approach for validating machine translation software. Our key insight is that the translation results of "similar" source sentences should typically exhibit similar sentence structures. Specifically, SIT (1) generates similar source sentences by substituting one word in a given sentence with semantically similar, syntactically equivalent words; (2) represents sentence structure by syntax parse trees (obtained via constituency or dependency parsing); (3) reports sentence pairs whose structures differ quantitatively by more than some threshold. To evaluate SIT, we use it to test Google Translate and Bing Microsoft Translator with 200 source sentences as input, which led to 64 and 70 buggy issues with 69.5$\backslash${\%} and 70$\backslash${\%} top-1 accuracy, respectively. The translation errors are diverse, including under-translation, over-translation, incorrect modification, word/phrase mistranslation, and unclear logic.},
archivePrefix = {arXiv},
arxivId = {1907.08710},
author = {He, Pinjia and Meister, Clara and Su, Zhendong},
eprint = {1907.08710},
file = {:home/wangpei/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He, Meister, Su - 2019 - Structure-Invariant Testing for Machine Translation.pdf:pdf},
keywords = {machine translation,metamorphic testing,structural invariance},
number = {1},
title = {{Structure-Invariant Testing for Machine Translation}},
url = {http://arxiv.org/abs/1907.08710},
year = {2019}
}
